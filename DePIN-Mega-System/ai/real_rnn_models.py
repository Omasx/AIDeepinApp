"""
ðŸ§  Real RNN/LSTM Models - Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ù…ØªÙƒØ±Ø±Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©
Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø¨Ø´ÙƒÙ„ Ø§Ø­ØªØ±Ø§ÙÙŠ

ÙŠØªØ¶Ù…Ù†:
- Ù†Ù…ÙˆØ°Ø¬ LSTM Ø£Ø³Ø§Ø³ÙŠ
- Ù†Ù…ÙˆØ°Ø¬ Bidirectional LSTM
- Ù†Ù…ÙˆØ°Ø¬ GRU
- Ù†Ù…ÙˆØ°Ø¬ Attention
- Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ÙØ¹Ù„ÙŠØ©
"""

import numpy as np
import logging
from typing import Tuple, List, Dict, Any, Optional
from dataclasses import dataclass

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers, models
    TF_AVAILABLE = True
except ImportError:
    logger.warning("âš ï¸ TensorFlow ØºÙŠØ± Ù…ØªÙˆÙØ±")
    TF_AVAILABLE = False


@dataclass
class RNNConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù†Ù…Ø§Ø°Ø¬ RNN"""
    vocab_size: int = 10000
    embedding_dim: int = 128
    max_length: int = 100
    lstm_units: int = 64
    dropout_rate: float = 0.5
    batch_size: int = 32
    epochs: int = 10


class RealRNNModels:
    """ÙØ¦Ø© Ù†Ù…Ø§Ø°Ø¬ RNN/LSTM Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©"""
    
    def __init__(self, config: RNNConfig = None):
        self.config = config or RNNConfig()
        self.models = {}
        self.tokenizer = None
        self.tf_available = TF_AVAILABLE
        
        logger.info(f"ðŸ§  ØªÙ‡ÙŠØ¦Ø© Ù†Ù…Ø§Ø°Ø¬ RNN - TensorFlow: {'Ù…ØªØ§Ø­' if self.tf_available else 'ØºÙŠØ± Ù…ØªØ§Ø­'}")
    
    def build_simple_lstm(self, name: str = "simple_lstm") -> Optional[Any]:
        """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ LSTM Ø¨Ø³ÙŠØ·"""
        
        if not self.tf_available:
            return None
        
        logger.info(f"ðŸ”¨ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ {name}")
        
        model = models.Sequential([
            # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†
            layers.Embedding(self.config.vocab_size, self.config.embedding_dim,
                            input_length=self.config.max_length),
            
            # Ø·Ø¨Ù‚Ø© LSTM Ø§Ù„Ø£ÙˆÙ„Ù‰
            layers.LSTM(self.config.lstm_units, return_sequences=True),
            layers.Dropout(self.config.dropout_rate),
            
            # Ø·Ø¨Ù‚Ø© LSTM Ø§Ù„Ø«Ø§Ù†ÙŠØ©
            layers.LSTM(self.config.lstm_units),
            layers.Dropout(self.config.dropout_rate),
            
            # Ø·Ø¨Ù‚Ø§Øª ÙƒØ§Ù…Ù„Ø©
            layers.Dense(64, activation='relu'),
            layers.Dropout(self.config.dropout_rate),
            layers.Dense(1, activation='sigmoid')  # Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ
        ])
        
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        self.models[name] = model
        logger.info(f"âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ {name}")
        
        return model
    
    def build_bidirectional_lstm(self, name: str = "bidirectional_lstm") -> Optional[Any]:
        """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Bidirectional LSTM"""
        
        if not self.tf_available:
            return None
        
        logger.info(f"ðŸ”¨ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ {name}")
        
        model = models.Sequential([
            # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†
            layers.Embedding(self.config.vocab_size, self.config.embedding_dim,
                            input_length=self.config.max_length),
            
            # Ø·Ø¨Ù‚Ø© Bidirectional LSTM
            layers.Bidirectional(
                layers.LSTM(self.config.lstm_units, return_sequences=True)
            ),
            layers.Dropout(self.config.dropout_rate),
            
            # Ø·Ø¨Ù‚Ø© Bidirectional LSTM Ø§Ù„Ø«Ø§Ù†ÙŠØ©
            layers.Bidirectional(
                layers.LSTM(self.config.lstm_units)
            ),
            layers.Dropout(self.config.dropout_rate),
            
            # Ø·Ø¨Ù‚Ø§Øª ÙƒØ§Ù…Ù„Ø©
            layers.Dense(128, activation='relu'),
            layers.Dropout(self.config.dropout_rate),
            layers.Dense(64, activation='relu'),
            layers.Dropout(self.config.dropout_rate),
            layers.Dense(1, activation='sigmoid')
        ])
        
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        self.models[name] = model
        logger.info(f"âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ {name}")
        
        return model
    
    def build_gru_model(self, name: str = "gru_model") -> Optional[Any]:
        """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ GRU"""
        
        if not self.tf_available:
            return None
        
        logger.info(f"ðŸ”¨ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ {name}")
        
        model = models.Sequential([
            # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†
            layers.Embedding(self.config.vocab_size, self.config.embedding_dim,
                            input_length=self.config.max_length),
            
            # Ø·Ø¨Ù‚Ø© GRU Ø§Ù„Ø£ÙˆÙ„Ù‰
            layers.GRU(self.config.lstm_units, return_sequences=True),
            layers.Dropout(self.config.dropout_rate),
            
            # Ø·Ø¨Ù‚Ø© GRU Ø§Ù„Ø«Ø§Ù†ÙŠØ©
            layers.GRU(self.config.lstm_units),
            layers.Dropout(self.config.dropout_rate),
            
            # Ø·Ø¨Ù‚Ø§Øª ÙƒØ§Ù…Ù„Ø©
            layers.Dense(64, activation='relu'),
            layers.Dropout(self.config.dropout_rate),
            layers.Dense(1, activation='sigmoid')
        ])
        
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        self.models[name] = model
        logger.info(f"âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ {name}")
        
        return model
    
    def build_attention_model(self, name: str = "attention_model") -> Optional[Any]:
        """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø¢Ù„ÙŠØ© Attention"""
        
        if not self.tf_available:
            return None
        
        logger.info(f"ðŸ”¨ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ {name} Ù…Ø¹ Attention")
        
        # Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        inputs = keras.Input(shape=(self.config.max_length,))
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†
        x = layers.Embedding(self.config.vocab_size, self.config.embedding_dim)(inputs)
        
        # Ø·Ø¨Ù‚Ø© LSTM
        x = layers.LSTM(self.config.lstm_units, return_sequences=True)(x)
        x = layers.Dropout(self.config.dropout_rate)(x)
        
        # Ø¢Ù„ÙŠØ© Attention
        attention = layers.MultiHeadAttention(
            num_heads=4,
            key_dim=self.config.lstm_units // 4
        )(x, x)
        
        # Ø¯Ù…Ø¬ Ù…Ø¹ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£ØµÙ„ÙŠ
        x = layers.Add()([x, attention])
        x = layers.LayerNormalization()(x)
        
        # Ø·Ø¨Ù‚Ø© LSTM Ø£Ø®Ø±Ù‰
        x = layers.LSTM(self.config.lstm_units)(x)
        x = layers.Dropout(self.config.dropout_rate)(x)
        
        # Ø·Ø¨Ù‚Ø§Øª ÙƒØ§Ù…Ù„Ø©
        x = layers.Dense(64, activation='relu')(x)
        x = layers.Dropout(self.config.dropout_rate)(x)
        outputs = layers.Dense(1, activation='sigmoid')(x)
        
        model = models.Model(inputs=inputs, outputs=outputs)
        
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        self.models[name] = model
        logger.info(f"âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ {name}")
        
        return model
    
    def build_text_generation_model(self, name: str = "text_generation") -> Optional[Any]:
        """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ"""
        
        if not self.tf_available:
            return None
        
        logger.info(f"ðŸ”¨ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ {name}")
        
        model = models.Sequential([
            # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†
            layers.Embedding(self.config.vocab_size, self.config.embedding_dim,
                            input_length=self.config.max_length - 1),
            
            # Ø·Ø¨Ù‚Ø§Øª LSTM
            layers.LSTM(128, return_sequences=True),
            layers.Dropout(0.3),
            
            layers.LSTM(128),
            layers.Dropout(0.3),
            
            # Ø·Ø¨Ù‚Ø§Øª ÙƒØ§Ù…Ù„Ø©
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.3),
            
            # Ø¥Ø®Ø±Ø§Ø¬: ØªÙˆÙ‚Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
            layers.Dense(self.config.vocab_size, activation='softmax')
        ])
        
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        self.models[name] = model
        logger.info(f"âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ {name}")
        
        return model
    
    def build_sequence_to_sequence(self, name: str = "seq2seq") -> Optional[Any]:
        """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Sequence-to-Sequence"""
        
        if not self.tf_available:
            return None
        
        logger.info(f"ðŸ”¨ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ {name}")
        
        # Ø§Ù„Ù…Ø´ÙØ± (Encoder)
        encoder_inputs = keras.Input(shape=(self.config.max_length,))
        encoder_embedding = layers.Embedding(self.config.vocab_size, 
                                            self.config.embedding_dim)(encoder_inputs)
        encoder_lstm = layers.LSTM(self.config.lstm_units, 
                                   return_state=True)
        encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)
        encoder_states = [state_h, state_c]
        
        # ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± (Decoder)
        decoder_inputs = keras.Input(shape=(self.config.max_length,))
        decoder_embedding = layers.Embedding(self.config.vocab_size,
                                            self.config.embedding_dim)(decoder_inputs)
        decoder_lstm = layers.LSTM(self.config.lstm_units, return_sequences=True,
                                   return_state=True)
        decoder_outputs, _, _ = decoder_lstm(decoder_embedding, 
                                            initial_state=encoder_states)
        decoder_dense = layers.Dense(self.config.vocab_size, activation='softmax')
        decoder_outputs = decoder_dense(decoder_outputs)
        
        # Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙƒØ§Ù…Ù„
        model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)
        
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        self.models[name] = model
        logger.info(f"âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ {name}")
        
        return model
    
    def train_model(self, model_name: str, X_train: np.ndarray, y_train: np.ndarray,
                   X_val: np.ndarray = None, y_val: np.ndarray = None,
                   epochs: int = None) -> Dict[str, Any]:
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        
        if not self.tf_available:
            return {'status': 'error', 'message': 'TensorFlow ØºÙŠØ± Ù…ØªØ§Ø­'}
        
        if model_name not in self.models:
            return {'status': 'error', 'message': f'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯'}
        
        model = self.models[model_name]
        epochs = epochs or self.config.epochs
        
        logger.info(f"ðŸŽ“ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ {model_name}")
        
        try:
            history = model.fit(
                X_train, y_train,
                batch_size=self.config.batch_size,
                epochs=epochs,
                validation_data=(X_val, y_val) if X_val is not None else None,
                verbose=1
            )
            
            return {
                'status': 'success',
                'model': model_name,
                'epochs': epochs,
                'history': history.history
            }
        
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£: {e}")
            return {'status': 'error', 'message': str(e)}
    
    def predict(self, model_name: str, X: np.ndarray) -> Dict[str, Any]:
        """Ø§Ù„ØªÙ†Ø¨Ø¤"""
        
        if not self.tf_available:
            return {'status': 'error', 'message': 'TensorFlow ØºÙŠØ± Ù…ØªØ§Ø­'}
        
        if model_name not in self.models:
            return {'status': 'error', 'message': f'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯'}
        
        try:
            model = self.models[model_name]
            predictions = model.predict(X, verbose=0)
            
            return {
                'status': 'success',
                'predictions': predictions.tolist(),
                'shape': predictions.shape
            }
        
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£: {e}")
            return {'status': 'error', 'message': str(e)}
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        
        if model_name not in self.models:
            return {'status': 'error', 'message': f'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯'}
        
        if not self.tf_available:
            return {'status': 'success', 'message': 'TensorFlow ØºÙŠØ± Ù…ØªØ§Ø­'}
        
        try:
            model = self.models[model_name]
            
            return {
                'status': 'success',
                'model': model_name,
                'num_layers': len(model.layers),
                'total_params': model.count_params(),
                'input_shape': str(model.input_shape),
                'output_shape': str(model.output_shape)
            }
        
        except Exception as e:
            return {'status': 'error', 'message': str(e)}


# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
async def main():
    """Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    
    print("\n" + "="*70)
    print("ðŸ§  Ù†Ù…Ø§Ø°Ø¬ RNN/LSTM Ø­Ù‚ÙŠÙ‚ÙŠØ©")
    print("="*70 + "\n")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    rnn = RealRNNModels()
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    print("ðŸ”¨ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...\n")
    
    rnn.build_simple_lstm("simple_lstm")
    rnn.build_bidirectional_lstm("bidirectional_lstm")
    rnn.build_gru_model("gru_model")
    rnn.build_attention_model("attention_model")
    rnn.build_text_generation_model("text_generation")
    rnn.build_sequence_to_sequence("seq2seq")
    
    # Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    print("\nðŸ“Š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬:\n")
    
    for model_name in ["simple_lstm", "bidirectional_lstm", "gru_model", 
                       "attention_model", "text_generation", "seq2seq"]:
        info = rnn.get_model_info(model_name)
        print(f"âœ… {model_name}: {info}")
    
    print("\n" + "="*70)
    print("âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ RNN Ø¨Ù†Ø¬Ø§Ø­!")
    print("="*70 + "\n")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
