"""
ğŸ“ Real Training System - Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ¹Ù„ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø´ÙƒÙ„ Ø§Ø­ØªØ±Ø§ÙÙŠ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©

ÙŠØªØ¶Ù…Ù†:
- Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ©
- ØªØ¯Ø±ÙŠØ¨ Ù…ØªÙ‚Ø¯Ù…
- Callbacks ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø©
- Ø­ÙØ¸ ÙˆØ§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ù†Ù‚Ø§Ø·
- Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
"""

import numpy as np
import logging
from typing import Dict, Any, Optional, Tuple, List
from dataclasses import dataclass
from datetime import datetime
import json
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import callbacks
    TF_AVAILABLE = True
except ImportError:
    logger.warning("âš ï¸ TensorFlow ØºÙŠØ± Ù…ØªÙˆÙØ±")
    TF_AVAILABLE = False


@dataclass
class TrainingConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    batch_size: int = 32
    epochs: int = 50
    learning_rate: float = 0.001
    validation_split: float = 0.2
    early_stopping_patience: int = 5
    reduce_lr_patience: int = 3
    checkpoint_dir: str = "./checkpoints"


class RealTrainingSystem:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ¹Ù„ÙŠ"""
    
    def __init__(self, config: TrainingConfig = None):
        self.config = config or TrainingConfig()
        self.tf_available = TF_AVAILABLE
        self.training_history = {}
        self.best_models = {}
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙØªÙŠØ´
        os.makedirs(self.config.checkpoint_dir, exist_ok=True)
        
        logger.info(f"ğŸ“ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ - TensorFlow: {'Ù…ØªØ§Ø­' if self.tf_available else 'ØºÙŠØ± Ù…ØªØ§Ø­'}")
    
    def prepare_data(self, X: np.ndarray, y: np.ndarray, 
                    validation_split: float = None) -> Tuple[Dict, Dict]:
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
        
        logger.info("ğŸ“Š ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        
        validation_split = validation_split or self.config.validation_split
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        num_samples = len(X)
        num_train = int(num_samples * (1 - validation_split))
        
        indices = np.random.permutation(num_samples)
        train_indices = indices[:num_train]
        val_indices = indices[num_train:]
        
        X_train, y_train = X[train_indices], y[train_indices]
        X_val, y_val = X[val_indices], y[val_indices]
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X_mean = X_train.mean()
        X_std = X_train.std()
        
        X_train = (X_train - X_mean) / (X_std + 1e-7)
        X_val = (X_val - X_mean) / (X_std + 1e-7)
        
        logger.info(f"âœ… ØªÙ… ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
        logger.info(f"   - Ø¹ÙŠÙ†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {len(X_train)}")
        logger.info(f"   - Ø¹ÙŠÙ†Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚: {len(X_val)}")
        
        return {
            'X_train': X_train,
            'y_train': y_train,
            'X_val': X_val,
            'y_val': y_val
        }, {
            'mean': X_mean,
            'std': X_std
        }
    
    def create_callbacks(self, model_name: str) -> List[Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Callbacks Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
        
        if not self.tf_available:
            return []
        
        logger.info(f"ğŸ”§ Ø¥Ù†Ø´Ø§Ø¡ Callbacks Ù„Ù€ {model_name}")
        
        checkpoint_path = os.path.join(
            self.config.checkpoint_dir,
            f"{model_name}_best.h5"
        )
        
        callback_list = [
            # Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
            callbacks.ModelCheckpoint(
                checkpoint_path,
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            ),
            
            # Ø¥ÙŠÙ‚Ø§Ù Ù…Ø¨ÙƒØ±
            callbacks.EarlyStopping(
                monitor='val_loss',
                patience=self.config.early_stopping_patience,
                verbose=1,
                restore_best_weights=True
            ),
            
            # ØªÙ‚Ù„ÙŠÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
            callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=self.config.reduce_lr_patience,
                min_lr=1e-7,
                verbose=1
            ),
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«
            callbacks.CSVLogger(
                os.path.join(self.config.checkpoint_dir, f"{model_name}_log.csv")
            ),
            
            # Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
            callbacks.LambdaCallback(
                on_epoch_end=self._on_epoch_end
            )
        ]
        
        return callback_list
    
    def _on_epoch_end(self, epoch: int, logs: Dict = None):
        """Ù…Ø¹Ø§Ù„Ø¬ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø­Ù‚Ø¨Ø©"""
        
        if logs is None:
            return
        
        if epoch % 10 == 0:
            logger.info(f"ğŸ“ˆ Ø§Ù„Ø­Ù‚Ø¨Ø© {epoch}: loss={logs.get('loss', 0):.4f}, "
                       f"val_loss={logs.get('val_loss', 0):.4f}")
    
    def train_model(self, model: Any, model_name: str, 
                   data: Dict, epochs: int = None,
                   verbose: int = 1) -> Dict[str, Any]:
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        
        if not self.tf_available:
            return {'status': 'error', 'message': 'TensorFlow ØºÙŠØ± Ù…ØªØ§Ø­'}
        
        logger.info(f"ğŸ“ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ {model_name}")
        
        epochs = epochs or self.config.epochs
        
        try:
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_train = data['X_train']
            y_train = data['y_train']
            X_val = data['X_val']
            y_val = data['y_val']
            
            # Ø¥Ù†Ø´Ø§Ø¡ Callbacks
            callback_list = self.create_callbacks(model_name)
            
            # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            start_time = datetime.now()
            
            history = model.fit(
                X_train, y_train,
                batch_size=self.config.batch_size,
                epochs=epochs,
                validation_data=(X_val, y_val),
                callbacks=callback_list,
                verbose=verbose
            )
            
            training_time = (datetime.now() - start_time).total_seconds()
            
            # Ø­ÙØ¸ Ø§Ù„Ø³Ø¬Ù„
            self.training_history[model_name] = {
                'history': history.history,
                'training_time': training_time,
                'epochs': epochs,
                'timestamp': datetime.now().isoformat()
            }
            
            logger.info(f"âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ {model_name} ÙÙŠ {training_time:.2f} Ø«Ø§Ù†ÙŠØ©")
            
            return {
                'status': 'success',
                'model': model_name,
                'epochs': epochs,
                'training_time': training_time,
                'final_loss': float(history.history['loss'][-1]),
                'final_val_loss': float(history.history['val_loss'][-1]),
                'history': history.history
            }
        
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {e}")
            return {'status': 'error', 'message': str(e)}
    
    def evaluate_model(self, model: Any, model_name: str,
                      X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Any]:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        
        if not self.tf_available:
            return {'status': 'error', 'message': 'TensorFlow ØºÙŠØ± Ù…ØªØ§Ø­'}
        
        logger.info(f"ğŸ“Š ØªÙ‚ÙŠÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ {model_name}")
        
        try:
            loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
            
            # Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¥Ø¶Ø§ÙÙŠØ©
            predictions = model.predict(X_test, verbose=0)
            
            logger.info(f"âœ… Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:")
            logger.info(f"   - Ø§Ù„Ø®Ø³Ø§Ø±Ø©: {loss:.4f}")
            logger.info(f"   - Ø§Ù„Ø¯Ù‚Ø©: {accuracy:.4f}")
            
            return {
                'status': 'success',
                'model': model_name,
                'loss': float(loss),
                'accuracy': float(accuracy),
                'predictions_shape': predictions.shape
            }
        
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: {e}")
            return {'status': 'error', 'message': str(e)}
    
    def hyperparameter_tuning(self, model_builder, X: np.ndarray, y: np.ndarray,
                             param_grid: Dict) -> Dict[str, Any]:
        """Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø©"""
        
        if not self.tf_available:
            return {'status': 'error', 'message': 'TensorFlow ØºÙŠØ± Ù…ØªØ§Ø­'}
        
        logger.info("ğŸ”§ Ø¨Ø¯Ø¡ Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø©")
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        data, _ = self.prepare_data(X, y)
        
        best_result = None
        best_params = None
        results = []
        
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø¨ÙƒÙŠ
        param_combinations = self._generate_param_combinations(param_grid)
        
        for i, params in enumerate(param_combinations):
            logger.info(f"\nğŸ” Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {i+1}/{len(param_combinations)}: {params}")
            
            try:
                # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                model = model_builder(params)
                
                # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
                history = model.fit(
                    data['X_train'], data['y_train'],
                    batch_size=self.config.batch_size,
                    epochs=10,  # Ø­Ù‚Ø¨ Ù‚Ù„ÙŠÙ„Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹
                    validation_data=(data['X_val'], data['y_val']),
                    verbose=0
                )
                
                # Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
                val_loss = history.history['val_loss'][-1]
                
                result = {
                    'params': params,
                    'val_loss': float(val_loss),
                    'history': history.history
                }
                
                results.append(result)
                
                logger.info(f"âœ… val_loss: {val_loss:.4f}")
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙØ¶Ù„
                if best_result is None or val_loss < best_result['val_loss']:
                    best_result = result
                    best_params = params
            
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£: {e}")
                continue
        
        logger.info(f"\nğŸ† Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª: {best_params}")
        
        return {
            'status': 'success',
            'best_params': best_params,
            'best_val_loss': best_result['val_loss'] if best_result else None,
            'total_trials': len(results),
            'successful_trials': len([r for r in results if r])
        }
    
    def _generate_param_combinations(self, param_grid: Dict) -> List[Dict]:
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª"""
        
        import itertools
        
        keys = param_grid.keys()
        values = param_grid.values()
        
        combinations = []
        for combination in itertools.product(*values):
            combinations.append(dict(zip(keys, combination)))
        
        return combinations
    
    def save_training_log(self, model_name: str, path: str) -> Dict[str, Any]:
        """Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        
        if model_name not in self.training_history:
            return {'status': 'error', 'message': f'Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø³Ø¬Ù„ Ù„Ù€ {model_name}'}
        
        try:
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(self.training_history[model_name], f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø³Ø¬Ù„: {path}")
            
            return {'status': 'success', 'path': path}
        
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£: {e}")
            return {'status': 'error', 'message': str(e)}
    
    def get_training_summary(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ø®Øµ Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        
        summary = {
            'total_models_trained': len(self.training_history),
            'models': {}
        }
        
        for model_name, history in self.training_history.items():
            summary['models'][model_name] = {
                'epochs': history.get('epochs', 0),
                'training_time': history.get('training_time', 0),
                'timestamp': history.get('timestamp', '')
            }
        
        return summary


# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
async def main():
    """Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    
    print("\n" + "="*70)
    print("ğŸ“ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ¹Ù„ÙŠ")
    print("="*70 + "\n")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    trainer = RealTrainingSystem()
    
    # ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ‡Ù…ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    print("ğŸ“Š ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±...\n")
    
    X = np.random.randn(1000, 28, 28, 1).astype(np.float32)
    y = np.random.randint(0, 10, 1000)
    y = keras.utils.to_categorical(y, 10)
    
    # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    data, normalization = trainer.prepare_data(X, y)
    
    print(f"\nâœ… ØªÙ… ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    print(f"   - Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„ØªØ·Ø¨ÙŠØ¹: mean={normalization['mean']:.4f}, std={normalization['std']:.4f}")
    
    # Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    print("\n" + "="*70)
    print("âœ… Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…!")
    print("="*70 + "\n")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
