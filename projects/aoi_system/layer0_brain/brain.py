import logging
import asyncio
from typing import List, Dict, Any

logger = logging.getLogger("AOI-Layer0-Brain")

class CoreBrain:
    """
    LAYER 0 â€“ Core Brain (ØªÙÙƒÙŠØ± ÙÙ‚Ø·)
    Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ©: Reasoning, Planning, Decision output
    """
    def __init__(self, model_name: str = "Llama 3.5"):
        self.model_name = model_name
        logger.info(f"ğŸ§  Brain Layer initialized with {self.model_name}")

    async def reason(self, prompt: str, context: Dict[str, Any] = None) -> str:
        """
        ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ù ÙˆØ¥Ø¹Ø·Ø§Ø¡ Ù‚Ø±Ø§Ø± (ØªÙÙƒÙŠØ± ÙÙ‚Ø·).
        """
        logger.info(f"ğŸ¤” Reasoning on: {prompt[:50]}...")

        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Ollama Ù…Ø­Ù„ÙŠØ§Ù‹ (Standard API)
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post("http://localhost:11434/api/generate", json={
                    "model": "llama3.5",
                    "prompt": prompt,
                    "stream": False
                }) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        return data.get("response", "No response from model.")
        except Exception as e:
            logger.warning(f"âš ï¸ Ollama connection failed, falling back to mock: {e}")

        # Ù…Ø­Ø§ÙƒØ§Ø© Ø±Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø­Ø§Ù„ ØºÙŠØ§Ø¨ Ollama
        await asyncio.sleep(1)
        return f"Decision based on {self.model_name}: Goal identified as feasible. Procedure: Systematic execution."

    async def generate_plan(self, goal: str, constraints: List[str] = None) -> List[Dict[str, Any]]:
        """
        ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‡Ø¯Ù Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù‡Ø§Ù… Ù…Ø¬Ø±Ø¯Ø©.
        """
        logger.info(f"ğŸ“‹ Generating plan for: {goal}")
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø®Ø·Ø© Ø¹Ù…Ù„
        return [
            {"step": 1, "task": "Analyze environment state", "required_tool": "system_monitor"},
            {"step": 2, "task": "Search for relevant data", "required_tool": "web_browser"},
            {"step": 3, "task": "Process information", "required_tool": "logic_engine"},
            {"step": 4, "task": "Finalize and report", "required_tool": "interface"}
        ]
